{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing imputed data\n",
    "\n",
    "USA_train_imputed = pd.read_csv(\"USA_train_imputed copy.csv\")\n",
    "USA_test_imputed = pd.read_csv(\"USA_test_imputed copy.csv\")\n",
    "USA_train_imputed.set_index('date', inplace=True)\n",
    "USA_test_imputed.set_index('date', inplace=True)\n",
    "\n",
    "Brazil_train_imputed = pd.read_csv(\"Brazil_train_imputed copy.csv\")\n",
    "Brazil_test_imputed = pd.read_csv(\"Brazil_test_imputed copy.csv\")\n",
    "Brazil_train_imputed.set_index('date', inplace=True)\n",
    "Brazil_test_imputed.set_index('date', inplace=True)\n",
    "\n",
    "Germany_train_imputed = pd.read_csv(\"Germany_train_imputed copy.csv\")\n",
    "Germany_test_imputed = pd.read_csv(\"Germany_test_imputed copy.csv\")\n",
    "Germany_train_imputed.set_index('date', inplace=True)\n",
    "Germany_test_imputed.set_index('date', inplace=True)\n",
    "\n",
    "Nigeria_train_imputed = pd.read_csv(\"Nigeria_train_imputed copy.csv\")\n",
    "Nigeria_test_imputed = pd.read_csv(\"Nigeria_test_imputed copy.csv\")\n",
    "Nigeria_train_imputed.set_index('date', inplace=True)\n",
    "Nigeria_test_imputed.set_index('date', inplace=True)\n",
    "\n",
    "China_train_imputed = pd.read_csv(\"China_train_imputed copy.csv\")\n",
    "China_test_imputed = pd.read_csv(\"China_test_imputed copy.csv\")\n",
    "China_train_imputed.set_index('date', inplace=True)\n",
    "China_test_imputed.set_index('date', inplace=True)\n",
    "\n",
    "Australia_train_imputed = pd.read_csv(\"Australia_train_imputed copy.csv\")\n",
    "Australia_test_imputed = pd.read_csv(\"Australia_test_imputed copy.csv\")\n",
    "Australia_train_imputed.set_index('date', inplace=True)\n",
    "Australia_test_imputed.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping columns\n",
    "After building the Prophet models, we determined that there were a number of variables in each country's data that had unique/constant values or were collinear with the target variable of 'new_cases'. We will drop these samme variables as well for developing the XGBoost model. You will see this later in the code, this is just an early note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding time lags as features\n",
    "lag_values = [1, 7, 14]\n",
    "\n",
    "for lag in lag_values:\n",
    "    # Create lag features for training set\n",
    "    for column in USA_train_imputed.columns:\n",
    "        if column != 'new_cases':  # Skip the target variable\n",
    "            USA_train_imputed[f'{column}_lag_{lag}'] = USA_train_imputed[column].shift(lag)\n",
    "\n",
    "    # Create lag features for test set\n",
    "    for column in USA_test_imputed.columns:\n",
    "        if column != 'new_cases':  # Skip the target variable\n",
    "            USA_test_imputed[f'{column}_lag_{lag}'] = USA_test_imputed[column].shift(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dataset for unique values in each column\n",
    "unique_counts = USA_train_imputed.nunique()\n",
    "\n",
    "# Identifying columns with no unique values\n",
    "columns_with_no_unique_values = unique_counts[unique_counts == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns based on work above\n",
    "USA_train_imputed = USA_train_imputed.drop(columns = columns_with_no_unique_values)\n",
    "USA_train_imputed = USA_train_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                            'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Fix Lag Columns so values at beginning are 0 instead of missing\n",
    "USA_train_imputed['new_cases_Lag_7'] = USA_train_imputed['new_cases_Lag_7'].fillna(0)\n",
    "USA_train_imputed['new_cases_Lag_14'] = USA_train_imputed['new_cases_Lag_14'].fillna(0)\n",
    "USA_train_imputed[\"Weekend\"] = USA_train_imputed[\"Weekend\"].astype(int)\n",
    "\n",
    "# Dropping the same columns for test dataset based on conclusions of train dataset\n",
    "USA_test_imputed = USA_test_imputed.drop(columns = columns_with_no_unique_values)\n",
    "USA_test_imputed = USA_test_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                            'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Ensuring that the lag column values in test dataset are those of last values in train\n",
    "USA_test_imputed['new_cases_Lag_7'][0:7] = USA_test_imputed['new_cases'][-7:]\n",
    "USA_test_imputed['new_cases_Lag_14'][0:14] = USA_test_imputed['new_cases'][-14:]\n",
    "USA_test_imputed[\"Weekend\"] = USA_test_imputed[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filling missing values of the lag features\n",
    "# def create_lag_features(df, lag_values=[1]):\n",
    "#     ret = df[['new_cases']]  # Assuming 'new_cases' is your target variable\n",
    "\n",
    "#     for lag in lag_values:\n",
    "#         for column in df.columns:\n",
    "#             if column != 'new_cases':  # Skip the target variable\n",
    "#                 lagdf = df[column].shift(lag)\n",
    "#                 lagdf.columns = [f'{column}_lag_{lag}']\n",
    "#                 ret = pd.concat([ret, lagdf], axis=1)\n",
    "#     return ret.fillna(0)\n",
    "\n",
    "# # Example for USA dataset with lag values 1, 7, 14, 30\n",
    "# lag_values = [1, 7, 14, 30]\n",
    "# USA_train_imputed_lagged = create_lag_features(USA_train_imputed, lag_values)\n",
    "# USA_test_imputed_lagged = create_lag_features(USA_test_imputed, lag_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values of the lag features\n",
    "def process(df, lag_values=[1]):\n",
    "    ret = df[['new_cases']]\n",
    "    for lag in lag_values:\n",
    "        lagdf = df.shift(lag)\n",
    "        lagdf.columns=[f'lag{lag}_' + str(col) for col in lagdf.columns]\n",
    "        ret=pd.concat([ret, lagdf], axis=1)\n",
    "    return ret.fillna(0) \n",
    "USA_train_imputed_lagged = process(USA_train_imputed, lag_values=[1,7,14,30])\n",
    "USA_test_imputed_lagged = process(USA_test_imputed, lag_values=[1,7,14,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_cases</th>\n",
       "      <th>lag1_stringency_index</th>\n",
       "      <th>lag1_new_cases</th>\n",
       "      <th>lag1_total_deaths_per_million</th>\n",
       "      <th>lag1_new_deaths</th>\n",
       "      <th>lag1_total_deaths</th>\n",
       "      <th>lag1_new_deaths_per_million</th>\n",
       "      <th>lag1_total_tests_per_thousand</th>\n",
       "      <th>lag1_new_tests</th>\n",
       "      <th>lag1_total_tests</th>\n",
       "      <th>...</th>\n",
       "      <th>lag30_total_vaccinations_lag_1_lag_7_lag_14</th>\n",
       "      <th>lag30_new_vaccinations_lag_1_lag_7_lag_14</th>\n",
       "      <th>lag30_new_vaccinations_smoothed_lag_1_lag_7_lag_14</th>\n",
       "      <th>lag30_new_people_vaccinated_smoothed_per_hundred_lag_1_lag_7_lag_14</th>\n",
       "      <th>lag30_new_people_vaccinated_smoothed_lag_1_lag_7_lag_14</th>\n",
       "      <th>lag30_new_vaccinations_smoothed_per_million_lag_1_lag_7_lag_14</th>\n",
       "      <th>lag30_new_cases_Lag_7_lag_1_lag_7_lag_14</th>\n",
       "      <th>lag30_new_cases_Lag_14_lag_1_lag_7_lag_14</th>\n",
       "      <th>lag30_Rolling_Mean_new_cases_lag_1_lag_7_lag_14</th>\n",
       "      <th>lag30_Weekend_lag_1_lag_7_lag_14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-26</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>509081.0</td>\n",
       "      <td>47.69</td>\n",
       "      <td>176889.0</td>\n",
       "      <td>2460.174</td>\n",
       "      <td>197.0</td>\n",
       "      <td>819029.0</td>\n",
       "      <td>0.592</td>\n",
       "      <td>2144.998</td>\n",
       "      <td>961271.0</td>\n",
       "      <td>714102068.0</td>\n",
       "      <td>...</td>\n",
       "      <td>437241247.0</td>\n",
       "      <td>1893414.0</td>\n",
       "      <td>1319843.0</td>\n",
       "      <td>0.085</td>\n",
       "      <td>281129.0</td>\n",
       "      <td>3975.0</td>\n",
       "      <td>95118.0</td>\n",
       "      <td>96372.0</td>\n",
       "      <td>72568.642857</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-28</th>\n",
       "      <td>356797.0</td>\n",
       "      <td>50.46</td>\n",
       "      <td>509081.0</td>\n",
       "      <td>2465.857</td>\n",
       "      <td>1892.0</td>\n",
       "      <td>820921.0</td>\n",
       "      <td>5.683</td>\n",
       "      <td>2150.086</td>\n",
       "      <td>1694071.0</td>\n",
       "      <td>715796139.0</td>\n",
       "      <td>...</td>\n",
       "      <td>438267689.0</td>\n",
       "      <td>1026442.0</td>\n",
       "      <td>1350210.0</td>\n",
       "      <td>0.094</td>\n",
       "      <td>312730.0</td>\n",
       "      <td>4067.0</td>\n",
       "      <td>33855.0</td>\n",
       "      <td>31825.0</td>\n",
       "      <td>72767.071429</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29</th>\n",
       "      <td>499452.0</td>\n",
       "      <td>50.46</td>\n",
       "      <td>356797.0</td>\n",
       "      <td>2473.078</td>\n",
       "      <td>2404.0</td>\n",
       "      <td>823325.0</td>\n",
       "      <td>7.221</td>\n",
       "      <td>2156.937</td>\n",
       "      <td>2280780.0</td>\n",
       "      <td>718076919.0</td>\n",
       "      <td>...</td>\n",
       "      <td>438818418.0</td>\n",
       "      <td>550729.0</td>\n",
       "      <td>1366855.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>330791.0</td>\n",
       "      <td>4117.0</td>\n",
       "      <td>32018.0</td>\n",
       "      <td>18560.0</td>\n",
       "      <td>74034.357143</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-30</th>\n",
       "      <td>589431.0</td>\n",
       "      <td>50.46</td>\n",
       "      <td>499452.0</td>\n",
       "      <td>2480.041</td>\n",
       "      <td>2318.0</td>\n",
       "      <td>825643.0</td>\n",
       "      <td>6.963</td>\n",
       "      <td>2163.853</td>\n",
       "      <td>2302224.0</td>\n",
       "      <td>720379143.0</td>\n",
       "      <td>...</td>\n",
       "      <td>440266939.0</td>\n",
       "      <td>1448521.0</td>\n",
       "      <td>1367682.0</td>\n",
       "      <td>0.108</td>\n",
       "      <td>357315.0</td>\n",
       "      <td>4119.0</td>\n",
       "      <td>111463.0</td>\n",
       "      <td>111113.0</td>\n",
       "      <td>73885.357143</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31</th>\n",
       "      <td>502588.0</td>\n",
       "      <td>50.46</td>\n",
       "      <td>589431.0</td>\n",
       "      <td>2484.670</td>\n",
       "      <td>1541.0</td>\n",
       "      <td>827184.0</td>\n",
       "      <td>4.629</td>\n",
       "      <td>2170.817</td>\n",
       "      <td>2318686.0</td>\n",
       "      <td>722697829.0</td>\n",
       "      <td>...</td>\n",
       "      <td>441888477.0</td>\n",
       "      <td>1621538.0</td>\n",
       "      <td>1386872.0</td>\n",
       "      <td>0.118</td>\n",
       "      <td>391216.0</td>\n",
       "      <td>4177.0</td>\n",
       "      <td>70948.0</td>\n",
       "      <td>69209.0</td>\n",
       "      <td>74653.142857</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710 rows Ã— 1325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            new_cases  lag1_stringency_index  lag1_new_cases  \\\n",
       "date                                                           \n",
       "2020-01-22        0.0                   0.00             0.0   \n",
       "2020-01-23        0.0                   0.00             0.0   \n",
       "2020-01-24        1.0                   0.00             0.0   \n",
       "2020-01-25        0.0                   0.00             1.0   \n",
       "2020-01-26        3.0                   0.00             0.0   \n",
       "...               ...                    ...             ...   \n",
       "2021-12-27   509081.0                  47.69        176889.0   \n",
       "2021-12-28   356797.0                  50.46        509081.0   \n",
       "2021-12-29   499452.0                  50.46        356797.0   \n",
       "2021-12-30   589431.0                  50.46        499452.0   \n",
       "2021-12-31   502588.0                  50.46        589431.0   \n",
       "\n",
       "            lag1_total_deaths_per_million  lag1_new_deaths  lag1_total_deaths  \\\n",
       "date                                                                            \n",
       "2020-01-22                          0.000              0.0                0.0   \n",
       "2020-01-23                          0.000              0.0                0.0   \n",
       "2020-01-24                          0.000              0.0                0.0   \n",
       "2020-01-25                          0.000              0.0                0.0   \n",
       "2020-01-26                          0.000              0.0                0.0   \n",
       "...                                   ...              ...                ...   \n",
       "2021-12-27                       2460.174            197.0           819029.0   \n",
       "2021-12-28                       2465.857           1892.0           820921.0   \n",
       "2021-12-29                       2473.078           2404.0           823325.0   \n",
       "2021-12-30                       2480.041           2318.0           825643.0   \n",
       "2021-12-31                       2484.670           1541.0           827184.0   \n",
       "\n",
       "            lag1_new_deaths_per_million  lag1_total_tests_per_thousand  \\\n",
       "date                                                                     \n",
       "2020-01-22                        0.000                          0.000   \n",
       "2020-01-23                        0.000                          0.000   \n",
       "2020-01-24                        0.000                          0.000   \n",
       "2020-01-25                        0.000                          0.000   \n",
       "2020-01-26                        0.000                          0.000   \n",
       "...                                 ...                            ...   \n",
       "2021-12-27                        0.592                       2144.998   \n",
       "2021-12-28                        5.683                       2150.086   \n",
       "2021-12-29                        7.221                       2156.937   \n",
       "2021-12-30                        6.963                       2163.853   \n",
       "2021-12-31                        4.629                       2170.817   \n",
       "\n",
       "            lag1_new_tests  lag1_total_tests  ...  \\\n",
       "date                                          ...   \n",
       "2020-01-22             0.0               0.0  ...   \n",
       "2020-01-23             0.0               0.0  ...   \n",
       "2020-01-24             0.0               0.0  ...   \n",
       "2020-01-25             0.0               0.0  ...   \n",
       "2020-01-26             0.0               0.0  ...   \n",
       "...                    ...               ...  ...   \n",
       "2021-12-27        961271.0       714102068.0  ...   \n",
       "2021-12-28       1694071.0       715796139.0  ...   \n",
       "2021-12-29       2280780.0       718076919.0  ...   \n",
       "2021-12-30       2302224.0       720379143.0  ...   \n",
       "2021-12-31       2318686.0       722697829.0  ...   \n",
       "\n",
       "            lag30_total_vaccinations_lag_1_lag_7_lag_14  \\\n",
       "date                                                      \n",
       "2020-01-22                                          0.0   \n",
       "2020-01-23                                          0.0   \n",
       "2020-01-24                                          0.0   \n",
       "2020-01-25                                          0.0   \n",
       "2020-01-26                                          0.0   \n",
       "...                                                 ...   \n",
       "2021-12-27                                  437241247.0   \n",
       "2021-12-28                                  438267689.0   \n",
       "2021-12-29                                  438818418.0   \n",
       "2021-12-30                                  440266939.0   \n",
       "2021-12-31                                  441888477.0   \n",
       "\n",
       "            lag30_new_vaccinations_lag_1_lag_7_lag_14  \\\n",
       "date                                                    \n",
       "2020-01-22                                        0.0   \n",
       "2020-01-23                                        0.0   \n",
       "2020-01-24                                        0.0   \n",
       "2020-01-25                                        0.0   \n",
       "2020-01-26                                        0.0   \n",
       "...                                               ...   \n",
       "2021-12-27                                  1893414.0   \n",
       "2021-12-28                                  1026442.0   \n",
       "2021-12-29                                   550729.0   \n",
       "2021-12-30                                  1448521.0   \n",
       "2021-12-31                                  1621538.0   \n",
       "\n",
       "            lag30_new_vaccinations_smoothed_lag_1_lag_7_lag_14  \\\n",
       "date                                                             \n",
       "2020-01-22                                                0.0    \n",
       "2020-01-23                                                0.0    \n",
       "2020-01-24                                                0.0    \n",
       "2020-01-25                                                0.0    \n",
       "2020-01-26                                                0.0    \n",
       "...                                                       ...    \n",
       "2021-12-27                                          1319843.0    \n",
       "2021-12-28                                          1350210.0    \n",
       "2021-12-29                                          1366855.0    \n",
       "2021-12-30                                          1367682.0    \n",
       "2021-12-31                                          1386872.0    \n",
       "\n",
       "            lag30_new_people_vaccinated_smoothed_per_hundred_lag_1_lag_7_lag_14  \\\n",
       "date                                                                              \n",
       "2020-01-22                                              0.000                     \n",
       "2020-01-23                                              0.000                     \n",
       "2020-01-24                                              0.000                     \n",
       "2020-01-25                                              0.000                     \n",
       "2020-01-26                                              0.000                     \n",
       "...                                                       ...                     \n",
       "2021-12-27                                              0.085                     \n",
       "2021-12-28                                              0.094                     \n",
       "2021-12-29                                              0.100                     \n",
       "2021-12-30                                              0.108                     \n",
       "2021-12-31                                              0.118                     \n",
       "\n",
       "            lag30_new_people_vaccinated_smoothed_lag_1_lag_7_lag_14  \\\n",
       "date                                                                  \n",
       "2020-01-22                                                0.0         \n",
       "2020-01-23                                                0.0         \n",
       "2020-01-24                                                0.0         \n",
       "2020-01-25                                                0.0         \n",
       "2020-01-26                                                0.0         \n",
       "...                                                       ...         \n",
       "2021-12-27                                           281129.0         \n",
       "2021-12-28                                           312730.0         \n",
       "2021-12-29                                           330791.0         \n",
       "2021-12-30                                           357315.0         \n",
       "2021-12-31                                           391216.0         \n",
       "\n",
       "            lag30_new_vaccinations_smoothed_per_million_lag_1_lag_7_lag_14  \\\n",
       "date                                                                         \n",
       "2020-01-22                                                0.0                \n",
       "2020-01-23                                                0.0                \n",
       "2020-01-24                                                0.0                \n",
       "2020-01-25                                                0.0                \n",
       "2020-01-26                                                0.0                \n",
       "...                                                       ...                \n",
       "2021-12-27                                             3975.0                \n",
       "2021-12-28                                             4067.0                \n",
       "2021-12-29                                             4117.0                \n",
       "2021-12-30                                             4119.0                \n",
       "2021-12-31                                             4177.0                \n",
       "\n",
       "            lag30_new_cases_Lag_7_lag_1_lag_7_lag_14  \\\n",
       "date                                                   \n",
       "2020-01-22                                       0.0   \n",
       "2020-01-23                                       0.0   \n",
       "2020-01-24                                       0.0   \n",
       "2020-01-25                                       0.0   \n",
       "2020-01-26                                       0.0   \n",
       "...                                              ...   \n",
       "2021-12-27                                   95118.0   \n",
       "2021-12-28                                   33855.0   \n",
       "2021-12-29                                   32018.0   \n",
       "2021-12-30                                  111463.0   \n",
       "2021-12-31                                   70948.0   \n",
       "\n",
       "            lag30_new_cases_Lag_14_lag_1_lag_7_lag_14  \\\n",
       "date                                                    \n",
       "2020-01-22                                        0.0   \n",
       "2020-01-23                                        0.0   \n",
       "2020-01-24                                        0.0   \n",
       "2020-01-25                                        0.0   \n",
       "2020-01-26                                        0.0   \n",
       "...                                               ...   \n",
       "2021-12-27                                    96372.0   \n",
       "2021-12-28                                    31825.0   \n",
       "2021-12-29                                    18560.0   \n",
       "2021-12-30                                   111113.0   \n",
       "2021-12-31                                    69209.0   \n",
       "\n",
       "            lag30_Rolling_Mean_new_cases_lag_1_lag_7_lag_14  \\\n",
       "date                                                          \n",
       "2020-01-22                                         0.000000   \n",
       "2020-01-23                                         0.000000   \n",
       "2020-01-24                                         0.000000   \n",
       "2020-01-25                                         0.000000   \n",
       "2020-01-26                                         0.000000   \n",
       "...                                                     ...   \n",
       "2021-12-27                                     72568.642857   \n",
       "2021-12-28                                     72767.071429   \n",
       "2021-12-29                                     74034.357143   \n",
       "2021-12-30                                     73885.357143   \n",
       "2021-12-31                                     74653.142857   \n",
       "\n",
       "            lag30_Weekend_lag_1_lag_7_lag_14  \n",
       "date                                          \n",
       "2020-01-22                                 0  \n",
       "2020-01-23                                 0  \n",
       "2020-01-24                                 0  \n",
       "2020-01-25                                 0  \n",
       "2020-01-26                                 0  \n",
       "...                                      ...  \n",
       "2021-12-27                             False  \n",
       "2021-12-28                              True  \n",
       "2021-12-29                              True  \n",
       "2021-12-30                             False  \n",
       "2021-12-31                             False  \n",
       "\n",
       "[710 rows x 1325 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USA_train_imputed_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training data\n",
    "X_train = USA_train_imputed_lagged.drop(columns=['new_cases'])\n",
    "y_train = USA_train_imputed_lagged['new_cases']\n",
    "\n",
    "# For testing data\n",
    "X_test = USA_test_imputed_lagged.drop(columns=['new_cases'])\n",
    "y_test = USA_test_imputed_lagged['new_cases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in c:\\users\\ryanc\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.5.0 in c:\\users\\ryanc\\anaconda3\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=1.1.1 in c:\\users\\ryanc\\anaconda3\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.17.3 in c:\\users\\ryanc\\anaconda3\\lib\\site-packages (from scikit-learn) (1.17.3)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\users\\ryanc\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.17.3 in c:\\users\\ryanc\\anaconda3\\lib\\site-packages (1.17.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.17.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 50 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n50 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 506, in inner_f\n    return f(**kwargs)\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 761, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 286, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 775, in <lambda>\n    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 506, in inner_f\n    return f(**kwargs)\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 616, in __init__\n    handle, feature_names, feature_types = dispatch_data_backend(\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\data.py\", line 707, in dispatch_data_backend\n    return _from_pandas_df(data, enable_categorical, missing, threads,\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\data.py\", line 297, in _from_pandas_df\n    data, feature_names, feature_types = _transform_pandas_df(\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\data.py\", line 242, in _transform_pandas_df\n    raise ValueError(msg + ', '.join(bad_fields))\nValueError: DataFrame.dtypes for data must be int, float, bool or category.  When\n                categorical type is supplied, DMatrix parameter `enable_categorical` must\n                be set to `True`.lag1_Weekend_lag_1, lag1_Weekend_lag_7, lag1_Weekend_lag_1_lag_7, lag1_Weekend_lag_14, lag1_Weekend_lag_1_lag_14, lag1_Weekend_lag_7_lag_14, lag1_Weekend_lag_1_lag_7_lag_14, lag7_Weekend_lag_1, lag7_Weekend_lag_7, lag7_Weekend_lag_1_lag_7, lag7_Weekend_lag_14, lag7_Weekend_lag_1_lag_14, lag7_Weekend_lag_7_lag_14, lag7_Weekend_lag_1_lag_7_lag_14, lag14_Weekend_lag_1, lag14_Weekend_lag_7, lag14_Weekend_lag_1_lag_7, lag14_Weekend_lag_14, lag14_Weekend_lag_1_lag_14, lag14_Weekend_lag_7_lag_14, lag14_Weekend_lag_1_lag_7_lag_14, lag30_Weekend_lag_1, lag30_Weekend_lag_7, lag30_Weekend_lag_1_lag_7, lag30_Weekend_lag_14, lag30_Weekend_lag_1_lag_14, lag30_Weekend_lag_7_lag_14, lag30_Weekend_lag_1_lag_7_lag_14\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-954a0e1b424c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mrandom_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Get the best hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    896\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1807\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1808\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1809\u001b[1;33m         evaluate_candidates(\n\u001b[0m\u001b[0;32m   1810\u001b[0m             ParameterSampler(\n\u001b[0;32m   1811\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    873\u001b[0m                     )\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 875\u001b[1;33m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m                 \u001b[1;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    412\u001b[0m                 \u001b[1;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             )\n\u001b[1;32m--> 414\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 50 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n50 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 506, in inner_f\n    return f(**kwargs)\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 761, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 286, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 775, in <lambda>\n    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 506, in inner_f\n    return f(**kwargs)\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 616, in __init__\n    handle, feature_names, feature_types = dispatch_data_backend(\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\data.py\", line 707, in dispatch_data_backend\n    return _from_pandas_df(data, enable_categorical, missing, threads,\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\data.py\", line 297, in _from_pandas_df\n    data, feature_names, feature_types = _transform_pandas_df(\n  File \"C:\\Users\\ryanc\\anaconda3\\lib\\site-packages\\xgboost\\data.py\", line 242, in _transform_pandas_df\n    raise ValueError(msg + ', '.join(bad_fields))\nValueError: DataFrame.dtypes for data must be int, float, bool or category.  When\n                categorical type is supplied, DMatrix parameter `enable_categorical` must\n                be set to `True`.lag1_Weekend_lag_1, lag1_Weekend_lag_7, lag1_Weekend_lag_1_lag_7, lag1_Weekend_lag_14, lag1_Weekend_lag_1_lag_14, lag1_Weekend_lag_7_lag_14, lag1_Weekend_lag_1_lag_7_lag_14, lag7_Weekend_lag_1, lag7_Weekend_lag_7, lag7_Weekend_lag_1_lag_7, lag7_Weekend_lag_14, lag7_Weekend_lag_1_lag_14, lag7_Weekend_lag_7_lag_14, lag7_Weekend_lag_1_lag_7_lag_14, lag14_Weekend_lag_1, lag14_Weekend_lag_7, lag14_Weekend_lag_1_lag_7, lag14_Weekend_lag_14, lag14_Weekend_lag_1_lag_14, lag14_Weekend_lag_7_lag_14, lag14_Weekend_lag_1_lag_7_lag_14, lag30_Weekend_lag_1, lag30_Weekend_lag_7, lag30_Weekend_lag_1_lag_7, lag30_Weekend_lag_14, lag30_Weekend_lag_1_lag_14, lag30_Weekend_lag_7_lag_14, lag30_Weekend_lag_1_lag_7_lag_14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the hyperparameter distributions\n",
    "param_dist = {\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'n_estimators': [50, 100, 200, 300, 500]\n",
    "}\n",
    "\n",
    "# Create XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Instantiate RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist, n_iter=10, scoring='neg_mean_squared_error', cv=5, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "final_model = XGBRegressor(**best_params)\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['iso_code', 'population_density', 'median_age', 'aged_65_older',\n",
      "       'aged_70_older', 'gdp_per_capita', 'extreme_poverty',\n",
      "       'cardiovasc_death_rate', 'hospital_beds_per_thousand',\n",
      "       'life_expectancy',\n",
      "       ...\n",
      "       'aged_70_older_lag_1_lag_7_lag_14', 'gdp_per_capita_lag_1_lag_7_lag_14',\n",
      "       'extreme_poverty_lag_1_lag_7_lag_14',\n",
      "       'cardiovasc_death_rate_lag_1_lag_7_lag_14',\n",
      "       'hospital_beds_per_thousand_lag_1_lag_7_lag_14',\n",
      "       'life_expectancy_lag_1_lag_7_lag_14',\n",
      "       'human_development_index_lag_1_lag_7_lag_14',\n",
      "       'location_lag_1_lag_7_lag_14', 'continent_lag_1_lag_7_lag_14',\n",
      "       'population_lag_1_lag_7_lag_14'],\n",
      "      dtype='object', length=112)\n"
     ]
    }
   ],
   "source": [
    "# Checking the dataset for unique values in each column\n",
    "unique_counts = USA_train_imputed.nunique()\n",
    "\n",
    "# Identifying columns with no unique values\n",
    "columns_with_no_unique_values = unique_counts[unique_counts == 1].index\n",
    "print(columns_with_no_unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the hyperparameter distributions\n",
    "param_dist = {\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'n_estimators': [50, 100, 200, 300, 500]\n",
    "}\n",
    "\n",
    "# Create XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Instantiate RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist, n_iter=10, scoring='root_mean_squared_error', cv=5, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "final_model = XGBRegressor(**best_params)\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "USA_multivariate_train_imputed['new_cases_Lag_7'] = USA_multivariate_train_imputed ['new_cases_Lag_7'].fillna(0)\n",
    "USA_multivariate_train_imputed['new_cases_Lag_14'] = USA_multivariate_train_imputed ['new_cases_Lag_14'].fillna(0)\n",
    "USA_multivariate_train_imputed['new_cases_Lag_1'] = USA_multivariate_train_imputed ['new_cases_Lag_1'].fillna(0)\n",
    "USA_multivariate_train_imputed['new_cases_Lag_2'] = USA_multivariate_train_imputed ['new_cases_Lag_2'].fillna(0)\n",
    "USA_multivariate_train_imputed[\"Weekend\"] = USA_multivariate_train_imputed[\"Weekend\"].astype(int)\n",
    "\n",
    "\n",
    "USA_multivariate_test_imputed = USA_test_imputed\n",
    "USA_multivariate_test_imputed = USA_multivariate_test_imputed.drop(columns=['iso_code', 'population_density'\n",
    "                                                                             ,'median_age', 'aged_65_older', 'aged_70_older',\n",
    "                                                                             'gdp_per_capita','extreme_poverty',\n",
    "                                                                              'cardiovasc_death_rate','hospital_beds_per_thousand',\n",
    "                                                                             'life_expectancy','human_development_index','total_cases',\n",
    "                                                                             'location', 'continent', 'population', 'reproduction_rate', 'new_cases_smoothed',\n",
    "                                                                           'total_cases_per_million', 'new_cases_per_million', 'new_cases_smoothed_per_million'])\n",
    "USA_multivariate_test_imputed['new_cases_Lag_7'][0:7] = USA_multivariate_train_imputed['new_cases'][-7:]\n",
    "USA_multivariate_test_imputed['new_cases_Lag_14'][0:14] = USA_multivariate_train_imputed['new_cases'][-14:]\n",
    "USA_multivariate_test_imputed['new_cases_Lag_1'][0:1] = USA_multivariate_train_imputed['new_cases'][-1:]\n",
    "USA_multivariate_test_imputed['new_cases_Lag_2'][0:2] = USA_multivariate_train_imputed['new_cases'][-2:]\n",
    "USA_multivariate_test_imputed[\"Weekend\"] = USA_multivariate_test_imputed[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for one country (USA)\n",
    "X_train = USA_train_imputed.drop('new_cases', axis=1)  # Features for training\n",
    "y_train = USA_train_imputed['new_cases']  # Target variable for training\n",
    "\n",
    "X_test = USA_test_imputed.drop('new_cases', axis=1)  # Features for testing\n",
    "y_test = USA_test_imputed['new_cases']  # Target variable for testing\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that have collinearity with 'new_cases'\n",
    "Brazil_train_imputed = Brazil_train_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                                      'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Fix Lag Columns so values at beginning are 0 instead of missing\n",
    "Brazil_train_imputed['new_cases_Lag_7'] = Brazil_train_imputed['new_cases_Lag_7'].fillna(0)\n",
    "Brazil_train_imputed['new_cases_Lag_14'] = Brazil_train_imputed['new_cases_Lag_14'].fillna(0)\n",
    "Brazil_train_imputed[\"Weekend\"] = Brazil_train_imputed[\"Weekend\"].astype(int)\n",
    "\n",
    "# Dropping the same columns for test dataset based on conclusions of train dataset\n",
    "Brazil_test_imputed = Brazil_test_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                            'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Ensuring that the lag column values in test dataset are those of last values in train\n",
    "Brazil_test_imputed['new_cases_Lag_7'][0:7] = Brazil_test_imputed['new_cases'][-7:]\n",
    "Brazil_test_imputed['new_cases_Lag_14'][0:14] = Brazil_test_imputed['new_cases'][-14:]\n",
    "Brazil_test_imputed[\"Weekend\"] = Brazil_test_imputed[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that have collinearity with 'new_cases'\n",
    "Germany_train_imputed = Germany_train_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                                      'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Fix Lag Columns so values at beginning are 0 instead of missing\n",
    "Germany_train_imputed['new_cases_Lag_7'] = Germany_train_imputed['new_cases_Lag_7'].fillna(0)\n",
    "Germany_train_imputed['new_cases_Lag_14'] = Germany_train_imputed['new_cases_Lag_14'].fillna(0)\n",
    "Germany_train_imputed[\"Weekend\"] = Germany_train_imputed[\"Weekend\"].astype(int)\n",
    "\n",
    "# Dropping the same columns for test dataset based on conclusions of train dataset\n",
    "Germany_test_imputed = Germany_test_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                            'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Ensuring that the lag column values in test dataset are those of last values in train\n",
    "Germany_test_imputed['new_cases_Lag_7'][0:7] = Germany_test_imputed['new_cases'][-7:]\n",
    "Germany_test_imputed['new_cases_Lag_14'][0:14] = Germany_test_imputed['new_cases'][-14:]\n",
    "Germany_test_imputed[\"Weekend\"] = Germany_test_imputed[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that have collinearity with 'new_cases'\n",
    "China_train_imputed = China_train_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                                      'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Fix Lag Columns so values at beginning are 0 instead of missing\n",
    "China_train_imputed['new_cases_Lag_7'] = China_train_imputed['new_cases_Lag_7'].fillna(0)\n",
    "China_train_imputed['new_cases_Lag_14'] = China_train_imputed['new_cases_Lag_14'].fillna(0)\n",
    "China_train_imputed[\"Weekend\"] = China_train_imputed[\"Weekend\"].astype(int)\n",
    "\n",
    "# Dropping the same columns for test dataset based on conclusions of train dataset\n",
    "China_test_imputed = China_test_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                            'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Ensuring that the lag column values in test dataset are those of last values in train\n",
    "China_test_imputed['new_cases_Lag_7'][0:7] = China_test_imputed['new_cases'][-7:]\n",
    "China_test_imputed['new_cases_Lag_14'][0:14] = China_test_imputed['new_cases'][-14:]\n",
    "China_test_imputed[\"Weekend\"] = China_test_imputed[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that have collinearity with 'new_cases'\n",
    "Nigeria_train_imputed = Nigeria_train_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                                      'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Fix Lag Columns so values at beginning are 0 instead of missing\n",
    "Nigeria_train_imputed['new_cases_Lag_7'] = Nigeria_train_imputed['new_cases_Lag_7'].fillna(0)\n",
    "Nigeria_train_imputed['new_cases_Lag_14'] = Nigeria_train_imputed['new_cases_Lag_14'].fillna(0)\n",
    "Nigeria_train_imputed[\"Weekend\"] = Nigeria_train_imputed[\"Weekend\"].astype(int)\n",
    "\n",
    "# Dropping the same columns for test dataset based on conclusions of train dataset\n",
    "Nigeria_test_imputed = Nigeria_test_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                            'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Ensuring that the lag column values in test dataset are those of last values in train\n",
    "Nigeria_test_imputed['new_cases_Lag_7'][0:7] = Nigeria_test_imputed['new_cases'][-7:]\n",
    "Nigeria_test_imputed['new_cases_Lag_14'][0:14] = Nigeria_test_imputed['new_cases'][-14:]\n",
    "Nigeria_test_imputed[\"Weekend\"] = Nigeria_test_imputed[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that have collinearity with 'new_cases'\n",
    "Australia_train_imputed = Australia_train_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                                      'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Fix Lag Columns so values at beginning are 0 instead of missing\n",
    "Australia_train_imputed['new_cases_Lag_7'] = Australia_train_imputed['new_cases_Lag_7'].fillna(0)\n",
    "Australia_train_imputed['new_cases_Lag_14'] = Australia_train_imputed['new_cases_Lag_14'].fillna(0)\n",
    "Australia_train_imputed[\"Weekend\"] = Australia_train_imputed[\"Weekend\"].astype(int)\n",
    "\n",
    "# Dropping the same columns for test dataset based on conclusions of train dataset\n",
    "Australia_test_imputed = Australia_test_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                            'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Ensuring that the lag column values in test dataset are those of last values in train\n",
    "Australia_test_imputed['new_cases_Lag_7'][0:7] = Australia_test_imputed['new_cases'][-7:]\n",
    "Australia_test_imputed['new_cases_Lag_14'][0:14] = Australia_test_imputed['new_cases'][-14:]\n",
    "Australia_test_imputed[\"Weekend\"] = Australia_test_imputed[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train datasets\n",
    "train_datasets = [USA_train_imputed, Brazil_train_imputed, Germany_train_imputed, Nigeria_train_imputed, China_train_imputed, Australia_train_imputed]\n",
    "composite_train = pd.concat(train_datasets, axis=0)\n",
    "\n",
    "# Resetting the index after concatenation\n",
    "composite_train.reset_index(inplace=True)\n",
    "\n",
    "# Combine test datasets\n",
    "test_datasets = [USA_test_imputed, Brazil_test_imputed, Germany_test_imputed, Nigeria_test_imputed, China_test_imputed, Australia_test_imputed]\n",
    "composite_test = pd.concat(test_datasets, axis=0)\n",
    "\n",
    "# Resetting the index after concatenation\n",
    "composite_test.reset_index(inplace=True)\n",
    "\n",
    "# Setting 'date' as the index for both composite_train and composite_test\n",
    "composite_train.set_index('date', inplace=True)\n",
    "composite_test.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping more columns, this time the ones with constant values as determined in the Prophet model development process\n",
    "composite_train = composite_train.drop(columns = ['population_density', 'population', 'location', 'continent',\n",
    "                                                  'life_expectancy', 'human_development_index', 'median_age', \n",
    "                                                  'aged_65_older', 'aged_70_older', 'gdp_per_capita', 'cardiovasc_death_rate', \n",
    "                                                  'extreme_poverty', 'hospital_beds_per_thousand', 'handwashing_facilities'])\n",
    "\n",
    "\n",
    "composite_test = composite_test.drop(columns = ['population_density', 'population', 'location', 'continent',\n",
    "                                                  'life_expectancy', 'human_development_index', 'median_age', \n",
    "                                                  'aged_65_older', 'aged_70_older', 'gdp_per_capita', 'cardiovasc_death_rate', \n",
    "                                                  'extreme_poverty', 'hospital_beds_per_thousand', 'handwashing_facilities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the time series data into a supervised learning format, considering time lags as features.\n",
    "def create_lag_features(df, columns, lag_values):\n",
    "    ret = df[columns].copy()\n",
    "    for column in columns:\n",
    "        for lag in lag_values:\n",
    "            lagged_column = df[column].shift(lag)\n",
    "            lagged_column.name = f'{column}_lag_{lag}'\n",
    "            ret = pd.concat([ret, lagged_column], axis=1)\n",
    "    return ret.fillna(0)\n",
    "\n",
    "# Specify the lag values explicitly. Using lags of 1,7, 14 & 30 lags for each feature.\n",
    "lag_values = [1, 7, 14, 30]  # Adjust as needed\n",
    "\n",
    "# List of dataframes for both train and test\n",
    "train_datasets = [composite_train]\n",
    "test_datasets = [composite_test]\n",
    "\n",
    "# Apply lag features to each train dataset\n",
    "for train_dataset in train_datasets:\n",
    "    train_dataset = create_lag_features(train_dataset, ['new_cases'], lag_values)\n",
    "\n",
    "# Apply lag features to each test dataset\n",
    "for test_dataset in test_datasets:\n",
    "    test_dataset = create_lag_features(test_dataset, ['new_cases'], lag_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# USA_train_imputed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_USA = xgb.XGBRegressor(learning_rate=0.01, max_depth=4, n_estimators=500, n_jobs=-1, random_state=0)\n",
    "m1_USA.fit(USA_train_imputed.drop(columns = ['new_cases', 'iso_code', 'location', 'continent'], axis=1), USA_train_imputed['new_cases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pd.DataFrame(m1_USA.predict(USA_train_imputed.drop(columns = ['new_cases', 'iso_code', 'location', 'continent'], axis=1)), columns=['yhat'], index=USA_train_imputed.index)\n",
    "train_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([USA_train_imputed['new_cases'],train_pred['yhat']], axis=1).plot(figsize=(15,5)) \n",
    "plt.legend(['USA_train_imputed', 'train_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Above models appears to overfit for USA train imputed dataset... Will need to check in with Rakin and Sid over the weekend"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
