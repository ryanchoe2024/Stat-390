{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing imputed data\n",
    "\n",
    "USA_train_imputed = pd.read_csv(\"USA_train_imputed copy.csv\")\n",
    "USA_test_imputed = pd.read_csv(\"USA_test_imputed copy.csv\")\n",
    "USA_train_imputed.set_index('date', inplace=True)\n",
    "USA_test_imputed.set_index('date', inplace=True)\n",
    "\n",
    "Brazil_train_imputed = pd.read_csv(\"Brazil_train_imputed copy.csv\")\n",
    "Brazil_test_imputed = pd.read_csv(\"Brazil_test_imputed copy.csv\")\n",
    "Brazil_train_imputed.set_index('date', inplace=True)\n",
    "Brazil_test_imputed.set_index('date', inplace=True)\n",
    "\n",
    "Germany_train_imputed = pd.read_csv(\"Germany_train_imputed copy.csv\")\n",
    "Germany_test_imputed = pd.read_csv(\"Germany_test_imputed copy.csv\")\n",
    "Germany_train_imputed.set_index('date', inplace=True)\n",
    "Germany_test_imputed.set_index('date', inplace=True)\n",
    "\n",
    "Nigeria_train_imputed = pd.read_csv(\"Nigeria_train_imputed copy.csv\")\n",
    "Nigeria_test_imputed = pd.read_csv(\"Nigeria_test_imputed copy.csv\")\n",
    "Nigeria_train_imputed.set_index('date', inplace=True)\n",
    "Nigeria_test_imputed.set_index('date', inplace=True)\n",
    "\n",
    "China_train_imputed = pd.read_csv(\"China_train_imputed copy.csv\")\n",
    "China_test_imputed = pd.read_csv(\"China_test_imputed copy.csv\")\n",
    "China_train_imputed.set_index('date', inplace=True)\n",
    "China_test_imputed.set_index('date', inplace=True)\n",
    "\n",
    "Australia_train_imputed = pd.read_csv(\"Australia_train_imputed copy.csv\")\n",
    "Australia_test_imputed = pd.read_csv(\"Australia_test_imputed copy.csv\")\n",
    "Australia_train_imputed.set_index('date', inplace=True)\n",
    "Australia_test_imputed.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping columns\n",
    "After building the Prophet models, we determined that there were a number of variables in each country's data that had unique/constant values or were collinear with the target variable of 'new_cases'. We will drop these samme variables as well for developing the XGBoost model. You will see this later in the code, this is just an early note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding time lags as features\n",
    "lag_values = [1, 7, 14]\n",
    "\n",
    "for lag in lag_values:\n",
    "    # Create lag features for training set\n",
    "    for column in USA_train_imputed.columns:\n",
    "        if column != 'new_cases':  # Skip the target variable\n",
    "            USA_train_imputed[f'{column}_lag_{lag}'] = USA_train_imputed[column].shift(lag)\n",
    "\n",
    "    # Create lag features for test set\n",
    "    for column in USA_test_imputed.columns:\n",
    "        if column != 'new_cases':  # Skip the target variable\n",
    "            USA_test_imputed[f'{column}_lag_{lag}'] = USA_test_imputed[column].shift(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns based on work above\n",
    "USA_train_imputed = USA_train_imputed.drop(columns = columns_with_no_unique_values)\n",
    "USA_train_imputed = USA_train_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                            'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Fix Lag Columns so values at beginning are 0 instead of missing\n",
    "USA_train_imputed['new_cases_Lag_7'] = USA_train_imputed['new_cases_Lag_7'].fillna(0)\n",
    "USA_train_imputed['new_cases_Lag_14'] = USA_train_imputed['new_cases_Lag_14'].fillna(0)\n",
    "USA_train_imputed[\"Weekend\"] = USA_train_imputed[\"Weekend\"].astype(int)\n",
    "\n",
    "# Dropping the same columns for test dataset based on conclusions of train dataset\n",
    "USA_test_imputed = USA_test_imputed.drop(columns = columns_with_no_unique_values)\n",
    "USA_test_imputed = USA_test_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                            'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Ensuring that the lag column values in test dataset are those of last values in train\n",
    "USA_test_imputed['new_cases_Lag_7'][0:7] = USA_test_imputed['new_cases'][-7:]\n",
    "USA_test_imputed['new_cases_Lag_14'][0:14] = USA_test_imputed['new_cases'][-14:]\n",
    "USA_test_imputed[\"Weekend\"] = USA_test_imputed[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filling missing values of the lag features\n",
    "# def create_lag_features(df, lag_values=[1]):\n",
    "#     ret = df[['new_cases']]  # Assuming 'new_cases' is your target variable\n",
    "\n",
    "#     for lag in lag_values:\n",
    "#         for column in df.columns:\n",
    "#             if column != 'new_cases':  # Skip the target variable\n",
    "#                 lagdf = df[column].shift(lag)\n",
    "#                 lagdf.columns = [f'{column}_lag_{lag}']\n",
    "#                 ret = pd.concat([ret, lagdf], axis=1)\n",
    "#     return ret.fillna(0)\n",
    "\n",
    "# # Example for USA dataset with lag values 1, 7, 14, 30\n",
    "# lag_values = [1, 7, 14, 30]\n",
    "# USA_train_imputed_lagged = create_lag_features(USA_train_imputed, lag_values)\n",
    "# USA_test_imputed_lagged = create_lag_features(USA_test_imputed, lag_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values of the lag features\n",
    "def process(df, lag_values=[1]):\n",
    "    ret = df[['new_cases']]\n",
    "    for lag in lag_values:\n",
    "        lagdf = df.shift(lag)\n",
    "        lagdf.columns=[f'lag{lag}_' + str(col) for col in lagdf.columns]\n",
    "        ret=pd.concat([ret, lagdf], axis=1)\n",
    "    return ret.fillna(0) \n",
    "USA_train_imputed_lagged = process(USA_train_imputed, lag_values=[1,7,14,30])\n",
    "USA_test_imputed_lagged = process(USA_test_imputed, lag_values=[1,7,14,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USA_train_imputed_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training data\n",
    "X_train = USA_train_imputed_lagged.drop(columns=['new_cases'])\n",
    "y_train = USA_train_imputed_lagged['new_cases']\n",
    "\n",
    "# For testing data\n",
    "X_test = USA_test_imputed_lagged.drop(columns=['new_cases'])\n",
    "y_test = USA_test_imputed_lagged['new_cases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.17.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the hyperparameter distributions\n",
    "param_dist = {\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'n_estimators': [50, 100, 200, 300, 500]\n",
    "}\n",
    "\n",
    "# Create XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Instantiate RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist, n_iter=10, scoring='neg_mean_squared_error', cv=5, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "final_model = XGBRegressor(**best_params)\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['iso_code', 'population_density', 'median_age', 'aged_65_older',\n",
      "       'aged_70_older', 'gdp_per_capita', 'extreme_poverty',\n",
      "       'cardiovasc_death_rate', 'hospital_beds_per_thousand',\n",
      "       'life_expectancy',\n",
      "       ...\n",
      "       'aged_70_older_lag_1_lag_7_lag_14', 'gdp_per_capita_lag_1_lag_7_lag_14',\n",
      "       'extreme_poverty_lag_1_lag_7_lag_14',\n",
      "       'cardiovasc_death_rate_lag_1_lag_7_lag_14',\n",
      "       'hospital_beds_per_thousand_lag_1_lag_7_lag_14',\n",
      "       'life_expectancy_lag_1_lag_7_lag_14',\n",
      "       'human_development_index_lag_1_lag_7_lag_14',\n",
      "       'location_lag_1_lag_7_lag_14', 'continent_lag_1_lag_7_lag_14',\n",
      "       'population_lag_1_lag_7_lag_14'],\n",
      "      dtype='object', length=112)\n"
     ]
    }
   ],
   "source": [
    "# Checking the dataset for unique values in each column\n",
    "unique_counts = USA_train_imputed.nunique()\n",
    "\n",
    "# Identifying columns with no unique values\n",
    "columns_with_no_unique_values = unique_counts[unique_counts == 1].index\n",
    "print(columns_with_no_unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the hyperparameter distributions\n",
    "param_dist = {\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'n_estimators': [50, 100, 200, 300, 500]\n",
    "}\n",
    "\n",
    "# Create XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Instantiate RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist, n_iter=10, scoring='root_mean_squared_error', cv=5, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "final_model = XGBRegressor(**best_params)\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "USA_multivariate_train_imputed['new_cases_Lag_7'] = USA_multivariate_train_imputed ['new_cases_Lag_7'].fillna(0)\n",
    "USA_multivariate_train_imputed['new_cases_Lag_14'] = USA_multivariate_train_imputed ['new_cases_Lag_14'].fillna(0)\n",
    "USA_multivariate_train_imputed['new_cases_Lag_1'] = USA_multivariate_train_imputed ['new_cases_Lag_1'].fillna(0)\n",
    "USA_multivariate_train_imputed['new_cases_Lag_2'] = USA_multivariate_train_imputed ['new_cases_Lag_2'].fillna(0)\n",
    "USA_multivariate_train_imputed[\"Weekend\"] = USA_multivariate_train_imputed[\"Weekend\"].astype(int)\n",
    "\n",
    "\n",
    "USA_multivariate_test_imputed = USA_test_imputed\n",
    "USA_multivariate_test_imputed = USA_multivariate_test_imputed.drop(columns=['iso_code', 'population_density'\n",
    "                                                                             ,'median_age', 'aged_65_older', 'aged_70_older',\n",
    "                                                                             'gdp_per_capita','extreme_poverty',\n",
    "                                                                              'cardiovasc_death_rate','hospital_beds_per_thousand',\n",
    "                                                                             'life_expectancy','human_development_index','total_cases',\n",
    "                                                                             'location', 'continent', 'population', 'reproduction_rate', 'new_cases_smoothed',\n",
    "                                                                           'total_cases_per_million', 'new_cases_per_million', 'new_cases_smoothed_per_million'])\n",
    "USA_multivariate_test_imputed['new_cases_Lag_7'][0:7] = USA_multivariate_train_imputed['new_cases'][-7:]\n",
    "USA_multivariate_test_imputed['new_cases_Lag_14'][0:14] = USA_multivariate_train_imputed['new_cases'][-14:]\n",
    "USA_multivariate_test_imputed['new_cases_Lag_1'][0:1] = USA_multivariate_train_imputed['new_cases'][-1:]\n",
    "USA_multivariate_test_imputed['new_cases_Lag_2'][0:2] = USA_multivariate_train_imputed['new_cases'][-2:]\n",
    "USA_multivariate_test_imputed[\"Weekend\"] = USA_multivariate_test_imputed[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for one country (USA)\n",
    "X_train = USA_train_imputed.drop('new_cases', axis=1)  # Features for training\n",
    "y_train = USA_train_imputed['new_cases']  # Target variable for training\n",
    "\n",
    "X_test = USA_test_imputed.drop('new_cases', axis=1)  # Features for testing\n",
    "y_test = USA_test_imputed['new_cases']  # Target variable for testing\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that have collinearity with 'new_cases'\n",
    "Brazil_train_imputed = Brazil_train_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                                      'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Fix Lag Columns so values at beginning are 0 instead of missing\n",
    "Brazil_train_imputed['new_cases_Lag_7'] = Brazil_train_imputed['new_cases_Lag_7'].fillna(0)\n",
    "Brazil_train_imputed['new_cases_Lag_14'] = Brazil_train_imputed['new_cases_Lag_14'].fillna(0)\n",
    "Brazil_train_imputed[\"Weekend\"] = Brazil_train_imputed[\"Weekend\"].astype(int)\n",
    "\n",
    "# Dropping the same columns for test dataset based on conclusions of train dataset\n",
    "Brazil_test_imputed = Brazil_test_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                            'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Ensuring that the lag column values in test dataset are those of last values in train\n",
    "Brazil_test_imputed['new_cases_Lag_7'][0:7] = Brazil_test_imputed['new_cases'][-7:]\n",
    "Brazil_test_imputed['new_cases_Lag_14'][0:14] = Brazil_test_imputed['new_cases'][-14:]\n",
    "Brazil_test_imputed[\"Weekend\"] = Brazil_test_imputed[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that have collinearity with 'new_cases'\n",
    "Germany_train_imputed = Germany_train_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                                      'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Fix Lag Columns so values at beginning are 0 instead of missing\n",
    "Germany_train_imputed['new_cases_Lag_7'] = Germany_train_imputed['new_cases_Lag_7'].fillna(0)\n",
    "Germany_train_imputed['new_cases_Lag_14'] = Germany_train_imputed['new_cases_Lag_14'].fillna(0)\n",
    "Germany_train_imputed[\"Weekend\"] = Germany_train_imputed[\"Weekend\"].astype(int)\n",
    "\n",
    "# Dropping the same columns for test dataset based on conclusions of train dataset\n",
    "Germany_test_imputed = Germany_test_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                            'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Ensuring that the lag column values in test dataset are those of last values in train\n",
    "Germany_test_imputed['new_cases_Lag_7'][0:7] = Germany_test_imputed['new_cases'][-7:]\n",
    "Germany_test_imputed['new_cases_Lag_14'][0:14] = Germany_test_imputed['new_cases'][-14:]\n",
    "Germany_test_imputed[\"Weekend\"] = Germany_test_imputed[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that have collinearity with 'new_cases'\n",
    "China_train_imputed = China_train_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                                      'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Fix Lag Columns so values at beginning are 0 instead of missing\n",
    "China_train_imputed['new_cases_Lag_7'] = China_train_imputed['new_cases_Lag_7'].fillna(0)\n",
    "China_train_imputed['new_cases_Lag_14'] = China_train_imputed['new_cases_Lag_14'].fillna(0)\n",
    "China_train_imputed[\"Weekend\"] = China_train_imputed[\"Weekend\"].astype(int)\n",
    "\n",
    "# Dropping the same columns for test dataset based on conclusions of train dataset\n",
    "China_test_imputed = China_test_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                            'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Ensuring that the lag column values in test dataset are those of last values in train\n",
    "China_test_imputed['new_cases_Lag_7'][0:7] = China_test_imputed['new_cases'][-7:]\n",
    "China_test_imputed['new_cases_Lag_14'][0:14] = China_test_imputed['new_cases'][-14:]\n",
    "China_test_imputed[\"Weekend\"] = China_test_imputed[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that have collinearity with 'new_cases'\n",
    "Nigeria_train_imputed = Nigeria_train_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                                      'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Fix Lag Columns so values at beginning are 0 instead of missing\n",
    "Nigeria_train_imputed['new_cases_Lag_7'] = Nigeria_train_imputed['new_cases_Lag_7'].fillna(0)\n",
    "Nigeria_train_imputed['new_cases_Lag_14'] = Nigeria_train_imputed['new_cases_Lag_14'].fillna(0)\n",
    "Nigeria_train_imputed[\"Weekend\"] = Nigeria_train_imputed[\"Weekend\"].astype(int)\n",
    "\n",
    "# Dropping the same columns for test dataset based on conclusions of train dataset\n",
    "Nigeria_test_imputed = Nigeria_test_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                            'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Ensuring that the lag column values in test dataset are those of last values in train\n",
    "Nigeria_test_imputed['new_cases_Lag_7'][0:7] = Nigeria_test_imputed['new_cases'][-7:]\n",
    "Nigeria_test_imputed['new_cases_Lag_14'][0:14] = Nigeria_test_imputed['new_cases'][-14:]\n",
    "Nigeria_test_imputed[\"Weekend\"] = Nigeria_test_imputed[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that have collinearity with 'new_cases'\n",
    "Australia_train_imputed = Australia_train_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                                      'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Fix Lag Columns so values at beginning are 0 instead of missing\n",
    "Australia_train_imputed['new_cases_Lag_7'] = Australia_train_imputed['new_cases_Lag_7'].fillna(0)\n",
    "Australia_train_imputed['new_cases_Lag_14'] = Australia_train_imputed['new_cases_Lag_14'].fillna(0)\n",
    "Australia_train_imputed[\"Weekend\"] = Australia_train_imputed[\"Weekend\"].astype(int)\n",
    "\n",
    "# Dropping the same columns for test dataset based on conclusions of train dataset\n",
    "Australia_test_imputed = Australia_test_imputed.drop(columns = ['total_cases_per_million', 'total_cases', 'new_cases_per_million',\n",
    "                                            'new_cases_smoothed_per_million', 'new_cases_smoothed', 'Rolling_Mean_new_cases'])\n",
    "\n",
    "# Ensuring that the lag column values in test dataset are those of last values in train\n",
    "Australia_test_imputed['new_cases_Lag_7'][0:7] = Australia_test_imputed['new_cases'][-7:]\n",
    "Australia_test_imputed['new_cases_Lag_14'][0:14] = Australia_test_imputed['new_cases'][-14:]\n",
    "Australia_test_imputed[\"Weekend\"] = Australia_test_imputed[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train datasets\n",
    "train_datasets = [USA_train_imputed, Brazil_train_imputed, Germany_train_imputed, Nigeria_train_imputed, China_train_imputed, Australia_train_imputed]\n",
    "composite_train = pd.concat(train_datasets, axis=0)\n",
    "\n",
    "# Resetting the index after concatenation\n",
    "composite_train.reset_index(inplace=True)\n",
    "\n",
    "# Combine test datasets\n",
    "test_datasets = [USA_test_imputed, Brazil_test_imputed, Germany_test_imputed, Nigeria_test_imputed, China_test_imputed, Australia_test_imputed]\n",
    "composite_test = pd.concat(test_datasets, axis=0)\n",
    "\n",
    "# Resetting the index after concatenation\n",
    "composite_test.reset_index(inplace=True)\n",
    "\n",
    "# Setting 'date' as the index for both composite_train and composite_test\n",
    "composite_train.set_index('date', inplace=True)\n",
    "composite_test.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping more columns, this time the ones with constant values as determined in the Prophet model development process\n",
    "composite_train = composite_train.drop(columns = ['population_density', 'population', 'location', 'continent',\n",
    "                                                  'life_expectancy', 'human_development_index', 'median_age', \n",
    "                                                  'aged_65_older', 'aged_70_older', 'gdp_per_capita', 'cardiovasc_death_rate', \n",
    "                                                  'extreme_poverty', 'hospital_beds_per_thousand', 'handwashing_facilities'])\n",
    "\n",
    "\n",
    "composite_test = composite_test.drop(columns = ['population_density', 'population', 'location', 'continent',\n",
    "                                                  'life_expectancy', 'human_development_index', 'median_age', \n",
    "                                                  'aged_65_older', 'aged_70_older', 'gdp_per_capita', 'cardiovasc_death_rate', \n",
    "                                                  'extreme_poverty', 'hospital_beds_per_thousand', 'handwashing_facilities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the time series data into a supervised learning format, considering time lags as features.\n",
    "def create_lag_features(df, columns, lag_values):\n",
    "    ret = df[columns].copy()\n",
    "    for column in columns:\n",
    "        for lag in lag_values:\n",
    "            lagged_column = df[column].shift(lag)\n",
    "            lagged_column.name = f'{column}_lag_{lag}'\n",
    "            ret = pd.concat([ret, lagged_column], axis=1)\n",
    "    return ret.fillna(0)\n",
    "\n",
    "# Specify the lag values explicitly. Using lags of 1,7, 14 & 30 lags for each feature.\n",
    "lag_values = [1, 7, 14, 30]  # Adjust as needed\n",
    "\n",
    "# List of dataframes for both train and test\n",
    "train_datasets = [composite_train]\n",
    "test_datasets = [composite_test]\n",
    "\n",
    "# Apply lag features to each train dataset\n",
    "for train_dataset in train_datasets:\n",
    "    train_dataset = create_lag_features(train_dataset, ['new_cases'], lag_values)\n",
    "\n",
    "# Apply lag features to each test dataset\n",
    "for test_dataset in test_datasets:\n",
    "    test_dataset = create_lag_features(test_dataset, ['new_cases'], lag_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# USA_train_imputed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_USA = xgb.XGBRegressor(learning_rate=0.01, max_depth=4, n_estimators=500, n_jobs=-1, random_state=0)\n",
    "m1_USA.fit(USA_train_imputed.drop(columns = ['new_cases', 'iso_code', 'location', 'continent'], axis=1), USA_train_imputed['new_cases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pd.DataFrame(m1_USA.predict(USA_train_imputed.drop(columns = ['new_cases', 'iso_code', 'location', 'continent'], axis=1)), columns=['yhat'], index=USA_train_imputed.index)\n",
    "train_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([USA_train_imputed['new_cases'],train_pred['yhat']], axis=1).plot(figsize=(15,5)) \n",
    "plt.legend(['USA_train_imputed', 'train_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Above models appears to overfit for USA train imputed dataset... Will need to check in with Rakin and Sid over the weekend"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
